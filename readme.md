# ğŸš€ LLM Pulse â€“ Intelligent Data Ingestion, Vectorization & Mock-Interview Reasoning System

LLM Pulse automates web content ingestion, vectorizes knowledge, and powers multiple downstream LLM applications â€” including **daily LLM updates retrieval**, **semantic search**, and a **Mock Interview system enhanced with CRAG (Corrective RAG)**.

---

# ğŸ“° 1. LLM Daily Updates Retrieval Pipeline

![alt text](flashcards.png)

This pipeline extracts **todayâ€™s newly ingested content**, summarizes it using LLMs, and delivers crisp daily updates for any user-selected topic.

## Workflow

```mermaid
flowchart TD
    A[User Query] --> B[Load FAISS Store]
    B --> C[Similarity Search]
    C --> D[Filter Docs by Today]
    D --> E[LLM Summaries]
    E --> F[Optional: Fact-Check]
    F --> G[Final Updates Returned]
```

### Output Example

```
Source: https://ragyfied.com/articles/attention-is-all-you-need-explained
Summary: Detailed breakdown of attention mechanisms in modern LLMs...
```

---

# ğŸ¯ 2. Mock Interview Pipeline 

* **Adaptive question generation**
* **Router Chain decision-making**
* **CRAG (Corrective Retrieval-Augmented Generation)**
* **Web + Vector + LLM reasoning fusion**

---

## ğŸ§  End-to-End Mock Interview Workflow

```mermaid
flowchart TD

A[User Selects Topic] --> B[Question Generator<br/>+ Difficulty Scoring]

B --> C[Router Chain<br/>LLM_ONLY vs VECTOR_DB]

C -->|LLM_ONLY| D[Direct LLM Answer]

C -->|VECTOR_DB| E[CRAG Pipeline]

E --> F1[Retrieve Top-k Docs]
E --> F2[Relevance Evaluation]
E --> F3{Decision}

F3 -->|Correct| G1[Use Best Retrieved Doc]
F3 -->|Ambiguous| G2[Combine Doc + Web Search]
F3 -->|Incorrect| G3[Web Search Only]

G1 --> H[Grounded Answer Generation]
G2 --> H
G3 --> H

D --> I[Final Answer Output]
H --> I
```

---

## ğŸ§© Core Components

### **1. Question Generator**

* Produces topic-aligned technical interview questions.
* Auto-assigns difficulty labels (Easy/Medium/Hard).
* Ensures coverage of both theory and practical depth.

### **2. Router Chain**

Optimizes cost + accuracy by making per-question decisions:

| Path        | When Used                                              |
| ----------- | ------------------------------------------------------ |
| `LLM_ONLY`  | The model can reliably answer from internal knowledge. |
| `VECTOR_DB` | When grounded retrieval is required.                   |

### **3. CRAG (Corrective RAG)**

Improves standard RAG by evaluating retrieval quality and applying corrections.

#### CRAG Decision Logic

| Relevance Score | Action                                       |
| --------------- | -------------------------------------------- |
| > 0.7           | Use document directly                        |
| 0.3â€“0.7         | Combine doc + refined knowledge + web search |
| < 0.3           | Skip docs â†’ web search                       |

---

## ğŸ“˜ Example Traces

### Router Output

```
LLM_ONLY â€” The model has sufficient internal knowledge for this question.
```

### CRAG Evaluation

```
Retrieved 7 documents
Scores: [0.2, 0.8, 0.4, 0.2, 0.2, 0.0, 0.2]
Decision: Correct â€” Using retrieved document
```

---

# ğŸ“¥ 3. LLM Data Ingestion & Vectorization Pipeline

A fully automated workflow that crawls new articles daily, deduplicates URLs, chunks text, generates embeddings, and stores everything inside a **FAISS vector database**.

### ğŸ”§ Key Capabilities

* Automated **daily crawling** of selected sources
* Deduplication via persistent `stored_urls.json`
* Batch-safe document loading
* Configurable chunking
* Embedding with `sentence-transformers/all-MiniLM-L6-v2`
* Incremental FAISS updates (no full rebuild)
* Detailed logging for reliability
* Designed for extensibility (multiple sources, custom loaders, new formats)

---

## ğŸ”„ Ingestion Workflow

```mermaid
flowchart TD
    A[Start: Load Existing URLs] --> B[Crawl Source for All Links]
    B --> C[Filter Only New URLs]
    C --> D[Save New URLs to stored_urls.json]
    D --> E{New URLs Exist?}
    E -->|No| F[Stop: No Updates Found]
    E -->|Yes| G[Load Documents in Batches]
    G --> H[Chunk Text]
    H --> I[Generate Embeddings]
    I --> J[Update FAISS Vector Store]
    J --> K[Pipeline Complete]
```

---

## ğŸ—‚ï¸ Vector Store Document Schema

Each FAISS entry is stored as:

```python
Document(
    id="uuid",
    metadata={
        "source": "...",
        "title": "...",
        "description": "...",
        "language": "en",
        "fetched_at": "YYYY-MM-DD"
    },
    page_content="Chunked document text..."
)
```

---

## â–¶ï¸ Running the Pipeline

```bash
pip install -r requirements.txt
python pipeline.py
```

---

# ğŸ“ Project Folder Structure

```
LLM-pulse/
â”‚
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ crawler.py
â”‚   â”œâ”€â”€ chunker.py
â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ vector_store_updater.py
â”‚
â”œâ”€â”€ latest_updates/
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ summarizer.py
â”‚
â”œâ”€â”€ mock_interview/
â”‚   â”œâ”€â”€ question_generator/
â”‚   â”‚   â””â”€â”€ generator.py
â”‚   â”œâ”€â”€ router/
â”‚   â”‚   â””â”€â”€ router.py
â”‚   â”œâ”€â”€ crag/
â”‚   â”‚   â””â”€â”€ crag.py
â”‚   â””â”€â”€ pipeline.py
â”‚
â”œâ”€â”€ llm_chains/
â”‚   â””â”€â”€ llm_chains.py
â”‚
â”œâ”€â”€ vectorstore/
â”‚   â””â”€â”€ vectorstore.py
â”‚
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ prompt_templates.py
â”‚
â”œâ”€â”€ constants.py
â”œâ”€â”€ embedder.py
â”œâ”€â”€ logging_config.py
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

